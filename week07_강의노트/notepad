형태소 분석

임베딩, 벡터의 형태로 단어를 표현

단어를 어떻게 표현할 것인가

학습 모델 ex)rnn 교체보다 임베딩 모델 교체하는 것이 성능 증가 폭이 훨씬 크다.

애플리케이션의 성능을 vector semantic으로 결정남

왜 벡터가 단어의 표현을 나타내며, 왜 벡터로 표현을 할까?

ex) fast is fimilar to rapid

온톨로지, 비슷한 단어간의 관계를 정의해놓은 DB

위의 예시처럼 두 단어간의 연관성 혹은 유사도를 어떻게 판단할 것인가.

기존의 wordnet처럼 단어의 관계를 정의한 DB는

사람이 제작하여 시간, 비용, 신뢰도에 문제가 있으며

새로운 단어 등 흐름에 적용하기 어렵다.



[슬라이드 3]

plagiarism detection (표절 검사)


[슬라이드 4]

차트와 같이 시간이 흐르며 단어의 의미가 바뀌는 것을 볼 수 있다.

(distributional model)



[슬라이드 5]

분산 모델은 단어의 사전적 의미가 아닌 텍스트에서 어떤 의미로 쓰이는지를 가지고 표현

'슬라이드 4'의 분모 모델처럼 비슷한 단어들 사이에서 벡터로 표시함

비슷한 단어 = 주변에 있는 단어 (유사어)로 판정 가능



[슬라이드 6]

tesquino 라는 단어의 의미 추측절차


[슬라이드 7]

두 단어의 벡터를 통해 유사도를 추정할 수 있다.

sparse vector
띄엄 띄엄, 0 사이에 의미있는 값이 분산

dense vector
밀집, 대부분 0이 아닌 의미있는 숫자로 채워져 있음

처음엔 sparse 벡터로 시작하여 dense 벡터로 변환

본래 매트릭스를 3개로 분해한 뒤 가공을 거쳐 재조립 하면 0이 거의 사라짐


word to vec -> 단어를 벡터로 변환하는 모델

1, 3번 (mutual-information ..., Neral - network....)


단어를 벡터로 표현하는 것을 word embedding이라 함

ex) Atree -> kobert

전산 언어학 = 텍스트 처리 = 자연어 처리의 응용에서 단어 리스트를 제작하여

행렬로 변환, 다룸




[슬라이드 8]
행렬로 임베딩 모델을 어떻게 만들까?

행렬의 행에는 긁어모은 텍스트의 타입 (단어), 열에는 신문 기사 혹은 문서의 제목을 놓고

단어가 등장한 빈도수를 행렬로 표기한다.

term frequency (tf 아래첨자t,d)

문서가 가질 수 있는 원소의 개수는 term (행의 개수)

N 위 첨자v = 자연수 N개로 이루어진 벡터
-> document를 표현하는 방법, 하나의 컬럼이 하나의 document를 의미함


단어 빈도수를 통해 두 문서의 유사도를 판단할 수 있다.


[슬라이드 9 ~ 12]

fool 의 대한 counter Vector N위첨자D는 37, 58, 1 ....

행으로 단어, 열로 문서 표현 가능


[슬라이드 13~15]

두 단어의 연관성을 보는 행렬

[슬라이드 14]

전체 문서가 아닌 작은단위의 문맥만 봄

단어 벡터의 사이즈는 더 이상 문서의 개수가 아닌 단어의 개수가 됨

ex) 매트릭스가 더 크다? = 단어가 더 sparse 하게 있다.


문장에서 해당 단어 주변 +-N 개만큼 좌우로 탐색하며 열에 해당하는 단어가 발견되면 카운트 증가

카운트가 높으면 연관성이 높은 단어로 판단

행렬의 분석에 따르면 기존의 단어에 대한 정의와 별개로 apricot(살구나무)와 pinch는 같은 의미의 단어로 판단한다.



[슬라이드 16]

50,000 * 50,000 의 행렬에서 상당수의 값이 0이라 해보자.

낭비가 너무 심함

컨텍스트 기준으로 카운트 할 때

목적에 따라 문장의 사이즈를 +- N개의 단어로 임의지정할 수 있다.


N이 크면 클 수록 의미적인 정보, 작을 수록 문법적인 정보를 나타낸다.

[슬라이드 17]

first - order c-o : 문법적으로 필적한 관계에 있는 단어를 추출

second - order c-o : 문장에서 의미가 비슷한 관계에 있는 단어를 추출

둘의 차이?

ex) write 와 문장 내에서 의미가 유사한지

- term vector, document vector -> 각각 metrics 추출
- 단어 수집 방식 N의 개수에 따른 의미
- first order : 문법적 관계, second order : 의미가 비슷한 관계

[슬라이드 18 ~ 19]

두 단어의 유사도 측정을 위한 방법

Measuring similarity: the cosine

여기서 cosine은 내적을 의미함

5만 , 5만의 벡터에서

서로 내적 연산한뒤 모두 더한 결과

이게 왜 좋은가? 같은 디멘션의 두 벡터가 비슷할 수록 큰 값이 나옴


[슬라이드 20]

20 페이지의 수식을 보면 값들의 크기와 상관없이 유사도가 더 중요하기 때문에

frequency에 따른 bias를 경감하기 위해 root를 취함

(한 단어가 너무 많이 나와 값이 편중되는 현상 방지)

[슬라이드 21 ~ 23]

두 벡터가 이루는 각 수식

cosine이 이루는 각을 통해 유사도 판단


내적을 기억해보자.
0~90도만 판단함


[슬라이드 24 ~ 25]

내적 계산, 그래프 그리기


다른 방법 : cosine, jaccard (최소값 / 최대값)



===============================

복습

같은 문맥의 다른 단어를 보면 해당 단어를 유추할 수 있다.

또한 주변단어들을 통해 단어의 벡터를 구할 수 있으며

표현이 가능함


term by term metrix 주변에 어느 단어가 몇 번이나 등장했는지 체크하여 단어의 벡터 측정

방향벡터의 데이터로 받아 내적 연산을 통해 일치율 검사

cosine 각을 통해 일치도 검사 가능 (이산값)


bag of word 방식 : 단어들간 순서나 관계없이 주변에 분포한 것만 봄

더 정교한 방식?


'이런 단어가 나타났는데 문법적 기능과 의미는 무엇을 뜻 하는가?'

ex) 두 단어가 서로 비슷한 단어의 주변에 있는 한정된 정보가 아닌
같은 문법으로 사용되고 비슷한 의미를 가진, 더 자세한 정보를 요구함

똑같은 단어라도 주어, 목적어 등 용법에 따라 해석이 달리므로

각 단어마다 데이터를 부여, 특징별로 벡터를 부여
(단어마다 모든 문법의 수를 검사)

따라서 벡터량이 기존(V)보다 추가한 경우의 수 R 배로 증가함

= 행렬이 sparse 해짐



Pado방법

V* (RV) 가 아닌 V*V를 그대로 사용

R과 독립적으로 계산하다 관계연산이 필요하면 따로 추출하여 연산?

-> 일괄적으로 R*V를 진행하면 사이즈가 너무 커지므로 필요한 경우에서만 추가 연산실행


[슬라이드 32]

tf-idf

Term frequency

많이 나타날 수록 더 많은 가중치 부여

다만! 많이 나오는 것은 좋은데 모든 문서에서 자주 등장하면 의미 없는 데이터!

전체 문서의 개수N을 i 단어가 나타난 문서의 수로 나눈 것을 log로 취한다.


단어와 document간의 관계 (얼마나 자주 나타나는가 = 중요한가)



[dense vector build-up : 슬라이드 34]
term by document metrix : 단어 문서 행렬 -> 단어의 개수 5만개, 문서의 개수 5만개 -> 수억개의 셀이 나오지만
대부분이 0임, 매트릭스가 커질 수록 데이터의 sparse가 증가함. 0의 비중도 점점 커지므로 공간의 낭비가 크다.

이처럼 벡터의 사이즈가 커질 수록 연산량이 크게 증가하므로 벡터의 사이즈를 줄이고 0이 아닌 데이터만 따로
모아줄 필요가 있다.


25억개의 쌍을 가진 매트릭스가 있다.

이중 0이 아닌 데이터는 1%도 미치지 못할 것.

즉, long-sparse한 벡터를 short-dense한 벡터로 바꾸어야 함
