{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt=Okt()\n",
    "\n",
    "file = open('./ratings_test.txt', 'r', encoding='utf-8-sig')\n",
    "\n",
    "sentences = []\n",
    "label = []\n",
    "\n",
    "for idx, line in enumerate(file):\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    line = line.split('\\t')\n",
    "    sentences.append(line[1])\n",
    "    label.append(line[2].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_pos = []\n",
    "\n",
    "for line in sentences:\n",
    "    sentences_pos.append(okt.morphs(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '....', '나쁘진', '않지만', '10', '점', '짜', '리', '는', '더', '더욱', '아니잖아'], ['지루하지는', '않은데', '완전', '막장', '임', '...', '돈', '주고', '보기', '에는', '....'], ['3', 'D', '만', '아니었어도', '별', '다섯', '개', '줬을텐데', '..', '왜', '3', 'D', '로', '나와서', '제', '심기', '를', '불편하게', '하죠', '??']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_pos[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(i) for i in sentences_pos])\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "for line in sentences_pos:\n",
    "    for word in line:\n",
    "        vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)+1\n",
    "\n",
    "vocab = sorted(list(vocab))\n",
    "\n",
    "vocab_index = {}\n",
    "for i in range(len(vocab)):\n",
    "    vocab_index[vocab[i]] = len(vocab_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나서써라', '나서야', '나서의', '나선', '나설', '나섰다가', '나성', '나셨어', '나소', '나수윤']\n",
      "9991\n"
     ]
    }
   ],
   "source": [
    "print(vocab[9990:10000])\n",
    "print(vocab_index['나서써라'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sentences = []\n",
    "\n",
    "for line in sentences_pos:\n",
    "    etc = []\n",
    "    for word in line:\n",
    "        etc.append(vocab_index[word])\n",
    "    int_sentences.append(etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7000, 3312], [1790], [21891, 34951, 40013, 52555, 16227, 39776, 406, 9955, 34363, 626, 44517, 47854, 18306, 12735, 14272, 14369, 33161], [47084, 34326, 38030, 18752, 41581, 378, 14894, 45887, 24632, 36379, 406], [1010, 1718, 18782, 33122, 24384, 13187, 4948, 46673, 346, 38146, 1010, 1718, 18049, 10265, 44855, 31885, 18300, 26512, 53798, 1567]]\n"
     ]
    }
   ],
   "source": [
    "print(int_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\python\\venv\\tensorflow\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from keras) (1.17.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from keras) (1.3.1)\n",
      "Requirement already satisfied: pyyaml in c:\\python\\venv\\tensorflow\\lib\\site-packages (from keras) (5.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: h5py in c:\\python\\venv\\tensorflow\\lib\\site-packages (from keras) (2.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/af/296748d4c8d8987423231b93aecce5ab5952f6f2243cb6cedb88dd425397/tensorflow-2.0.0-cp36-cp36m-win_amd64.whl (48.1MB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Collecting gast==0.2.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (1.17.1)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz (69kB)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/03/7d/5a51d7116a0f5a3a916148ed495698bfb6800868f0f60b0af55cb8353e84/grpcio-1.25.0-cp36-cp36m-win_amd64.whl (1.9MB)\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz (103kB)\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting wheel>=0.26 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl\n",
      "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/9e/a48cd34dd7b672ffc227b566f7d16d63c62c58b542d54efa45848c395dd4/tensorboard-2.0.1-py3-none-any.whl (3.8MB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Collecting wrapt>=1.11.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/2d/73/4a14606fa26f186e23015bc974f9010e2bbf1607f372e3bd5e82d2a62f1b/protobuf-3.10.0-cp36-cp36m-win_amd64.whl (1.1MB)\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl (57kB)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: h5py in c:\\python\\venv\\tensorflow\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting setuptools>=41.0.0 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/de/554b6310ac87c5b921bc45634b07b11394fe63bc4cb5176f5240addf18ab/setuptools-41.6.0-py2.py3-none-any.whl (582kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/81/d1e7d9974ba7c886f6d133a8baae18cb8d92b2d09bcc4f46328306825de0/google_auth-1.7.0-py2.py3-none-any.whl (74kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting cachetools<3.2,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
      "Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.22.0)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/71/8f0d444e3a74e5640a3d5d967c1c6b015da9c655f35b2d308a55d907a517/pyasn1-0.4.7-py2.py3-none-any.whl (76kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.25.6)\n",
      "Installing collected packages: gast, opt-einsum, grpcio, absl-py, tensorflow-estimator, termcolor, wheel, werkzeug, setuptools, protobuf, oauthlib, requests-oauthlib, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, google-auth-oauthlib, markdown, tensorboard, wrapt, google-pasta, astor, tensorflow\n",
      "  Running setup.py install for gast: started\n",
      "    Running setup.py install for gast: finished with status 'done'\n",
      "  Running setup.py install for opt-einsum: started\n",
      "    Running setup.py install for opt-einsum: finished with status 'done'\n",
      "  Running setup.py install for absl-py: started\n",
      "    Running setup.py install for absl-py: finished with status 'done'\n",
      "  Running setup.py install for termcolor: started\n",
      "    Running setup.py install for termcolor: finished with status 'done'\n",
      "  Found existing installation: setuptools 40.6.2\n",
      "    Uninstalling setuptools-40.6.2:\n",
      "      Successfully uninstalled setuptools-40.6.2\n",
      "  Running setup.py install for wrapt: started\n",
      "    Running setup.py install for wrapt: finished with status 'done'\n",
      "Successfully installed absl-py-0.8.1 astor-0.8.0 cachetools-3.1.1 gast-0.2.2 google-auth-1.7.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 markdown-3.1.1 oauthlib-3.1.0 opt-einsum-3.1.0 protobuf-3.10.0 pyasn1-0.4.7 pyasn1-modules-0.2.7 requests-oauthlib-1.3.0 rsa-4.0 setuptools-41.6.0 tensorboard-2.0.1 tensorflow-2.0.0 tensorflow-estimator-2.0.1 termcolor-1.1.0 werkzeug-0.16.0 wheel-0.33.6 wrapt-1.11.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padding_sentences = []\n",
    "int_sentences = pad_sequences(int_sentences, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['음악', '이', '주가', '된', ',', '최고', '의', '음악', '영화']\n",
      "[39881 40013 45872 15654   203 49613 39934 39881 37170     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_pos[5])\n",
    "print(int_sentences[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "label_one_hot = to_categorical(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(label_one_hot[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_etc, y_train, y_etc = train_test_split(int_sentences, label_one_hot, test_size = 0.4, random_state = 42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_etc, y_etc, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_val))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 82)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 82, 64)            3635968   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5248)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                335936    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 3,976,194\n",
      "Trainable params: 3,976,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Input, Dense, Flatten #keras에서도 임베딩 레이어를 제공함\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "embedding_size = 64\n",
    "\n",
    "input_data = Input(shape=(max_len, ))\n",
    "\n",
    "emd = Embedding(vocab_size, 64)(input_data)\n",
    "\n",
    "flatten = Flatten()(emd)\n",
    "\n",
    "dense1 = Dense(64, activation='relu')(flatten)\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "dense3 = Dense(2, activation='softmax')(dense2)\n",
    "\n",
    "model = Model(inputs=input_data, outputs=dense3)\n",
    "model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\venv\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "30000/30000 [==============================] - 12s 406us/step - loss: 0.5754 - accuracy: 0.6706 - val_loss: 0.4729 - val_accuracy: 0.7715\n",
      "Epoch 2/8\n",
      "30000/30000 [==============================] - 12s 396us/step - loss: 0.2745 - accuracy: 0.8881 - val_loss: 0.4140 - val_accuracy: 0.8173\n",
      "Epoch 3/8\n",
      "30000/30000 [==============================] - 12s 396us/step - loss: 0.1013 - accuracy: 0.9669 - val_loss: 0.5488 - val_accuracy: 0.8030\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1e96ccfa080>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose=1)\n",
    "\n",
    "model.fit([X_train], y_train, batch_size=256, epochs=8, validation_data=([X_val], y_val), callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 48us/step\n",
      "Loss: 0.5649034008979797\n",
      "Accuracy: 0.8004999756813049\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate([X_test], y_test)\n",
    "\n",
    "print('Loss: '+str(evaluation[0]))\n",
    "print('Accuracy: '+str(evaluation[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
